> üöÄ Este projeto foi atualizado com uma nova vers√£o que utiliza ferramentas modernas como **API para ingest√£o de dados, dbt, Airflow** e uma arquitetura mais robusta.  
> Acesse a nova vers√£o na branch [`feature/revamp-2025`](https://github.com/nayyarabernardo/etl-dados-cat-inss/tree/feature/revamp-2025) para ver as melhorias!

# Projeto ETL: dados de Comunica√ß√£o de Acidentes do Trabalho do INSS (CATWEB) üë∑

Este projeto tem como objetivo consolidar as informa√ß√µes dispon√≠veis no sistema informatizado de Comunica√ß√£o de Acidentes do Trabalho do INSS (CATWEB), referentes ao ano de 2022. Os dados ser√£o processados utilizando t√©cnicas de extra√ß√£o, transforma√ß√£o e carregamento (ETL), utilizando ferramentas como Colab e Google Cloud.

## √çndice

- [Escopo do Projeto Project](#escopo-do-projeto-project)
- [Ingest√£o](#ingest√£o)
- [Considera√ß√µes](#considera√ß√µes)
- [Contribui√ß√£o](#contribui√ß√£o)
- [Refer√™ncias](#refer√™ncias)

## Escopo do Projeto Project

O objetivo do projeto √© consolidar os dados dispon√≠veis no sistema informatizado de Comunica√ß√£o de Acidentes do Trabalho do INSS (CATWEB), referentes ao ano de 2022, e realizar a sua limpeza e organiza√ß√£o. Os dados ser√£o salvos em cole√ß√µes diferentes no MongoDB Atlas e em um bucket do CloudStorage, e o dataset final ser√° disponibilizado em um mysql. O projeto ser√° desenvolvido em tr√™s n√≠veis: infraestrutura, pandas e PySpark.
Passos para orientar o projeto

### Defini√ß√£o do escopo do projeto:
- Definir quais dados ser√£o extra√≠dos do sistema CATWEB
- Estabelecer quais ferramentas e tecnologias ser√£o utilizadas no processo de ETL
- Identificar poss√≠veis problemas de qualidade de dados e definir estrat√©gias para trat√°-los

#### Ingest√£o:
- Coletar dados do sistema CATWEB
- Armazenar dados em um banco de dados MongoDB Atlas
- Salvar datasets em um bucket do Google Cloud Storage

#### Processamento:
- Extrair dados de diferentes fontes
- Limpar e tratar os dados, eliminando valores faltantes ou inconsistentes
- Realizar opera√ß√µes de agrega√ß√£o, filtragem e ordena√ß√£o
- Criar novas colunas para facilitar a an√°lise dos dados

#### Armazenamento:
- Salvar o dataset final em um banco de dados MySQL
Habilidades t√©cnicas utilizadas no projeto

As habilidades t√©cnicas utilizadas neste projeto incluem:

    - Python
    - Pandas
    - PySpark
    - Google Cloud Platform (GCP)
    - MongoDB Atlas
    - MySQL

## Etapas do projeto
### Definir escopo do projeto e coletar dados

    Identificar os dados que ser√£o extra√≠dos do sistema CATWEB
    Definir as ferramentas e tecnologias que ser√£o utilizadas no processo de ETL
    Coletar e armazenar os dados em um banco de dados MongoDB Atlas e em um bucket do Google Cloud Storage

### Explorar e acessar os dados

    Extrair os dados do MongoDB Atlas e do bucket do Google Cloud Storage
    Carregar os dados em um dataframe do Pandas e em um dataframe do PySpark
    Realizar a limpeza e tratamento dos dados, eliminando valores faltantes ou inconsistentes
    Realizar opera√ß√µes de agrega√ß√£o, filtragem e ordena√ß√£o
    Criar novas colunas para facilitar a an√°lise dos dados

### Completar a escrita do projeto

    Salvar o dataset final em um banco de dados MySQL
    Realizar a an√°lise explorat√≥ria dos dados, identificando padr√µes e insights relevantes
    Documentar o processo de ETL e an√°lise de dados em um notebook do Colab ou em um arquivo README.md
    Compartilhar o dataset final e o notebook com outros usu√°rios interessados na an√°lise dos dados de acidentes do trabalho

## Ingest√£o

A extra√ß√£o dos dados ser√° realizada diretamente do sistema informatizado de Comunica√ß√£o de Acidentes do Trabalho do INSS (CATWEB), utilizando t√©cnicas de web scraping. Os dados ser√£o baixados em formato CSV e posteriormente carregados no ambiente de processamento.

## Processing

### N√≠vel Infra

No n√≠vel de infraestrutura, o arquivo original e tratado ser√° salvo em MongoDB Atlas em cole√ß√µes diferentes, com a identifica√ß√£o do final tratado ou original. Os datasets ser√£o obrigatoriamente salvos em um bucket do CloudStorage, tanto na vers√£o original quanto na tratada. O dataset final ser√° disponibilizado em um mysql.
### N√≠vel Pandas

No n√≠vel Pandas, ser√° realizada a extra√ß√£o dos dados para um dataframe, com a verifica√ß√£o da exist√™ncia de dados inconsistentes e a realiza√ß√£o da limpeza para NaN/NA ou algum valor atribu√≠do pelo analista, com a explica√ß√£o da decis√£o tomada. Ser√° realizada a exclus√£o de colunas do dataframe, caso necess√°rio, com a realiza√ß√£o do coment√°rio explicando a exclus√£o. Ser√£o agregados todos os dataframes originais em um √∫nico dataframe tratado. 

### N√≠vel PySpark

No n√≠vel PySpark, ser√° montada a estrutura do dataframe utilizando o StructType, com a verifica√ß√£o da exist√™ncia de dados inconsistentes, nulos e a realiza√ß√£o da limpeza. Ser√° verificada a necessidade de drop em colunas ou linhas, caso necess√°rio, com a realiza√ß√£o do coment√°rio explicando a exclus√£o. Ser√° realizada a mudan√ßa de nome de pelo menos 2 colunas. Ser√£o criadas pelo menos duas novas colunas contendo alguma informa√ß√£o relevante sobre as outras colunas j√° existentes, utilizando fun√ß√µes de agrupamento, agrega√ß√£o ou joins. Ser√£o utilizados filtros, ordena√ß√£o e agrupamento, trazendo dados relevantes para o neg√≥cio em quest√£o. Ser√£o utilizadas pelo menos duas Window Functions.

## Storing

Os dados ser√£o salvos em MongoDB Atlas em cole√ß√µes diferentes, com a identifica√ß√£o do final tratado ou original. Os datasets ser√£o obrigatoriamente salvos em um bucket do CloudStorage, tanto na vers√£o original quanto na tratada. O dataset final ser√° disponibilizado em um mysql.

## Considera√ß√µes

O processo de ETL pode ser uma tarefa complexa, mas √© fundamental para garantir a qualidade e consist√™ncia dos dados. Ao utilizar ferramentas como o Colab e o Google Cloud, podemos simplificar e automatizar muitas das etapas envolvidas. A limpeza e transforma√ß√£o dos dados foram etapas cr√≠ticas para obter insights significativos. Durante este projeto, foram identificados e tratados v√°rios problemas, como valores nulos, dados inconsistentes e colunas desnecess√°rias. 
As tr√™s insights identificadas neste projeto 
> as profiss√µes com maior incid√™ncia de acidentes, 

> as causas mais comuns de acidentes 

> e os estados com mais registros de acidentes

podem ser √∫teis para diferentes p√∫blicos, como profissionais de seguran√ßa do trabalho, empresas e institui√ß√µes governamentais.

## Contribui√ß√£o

Este projeto √© aberto a contribui√ß√µes. Se voc√™ deseja melhorar ou adicionar recursos, sinta-se √† vontade para criar uma solicita√ß√£o pull ou entrar em [contato](https://www.linkedin.com/in/nayyarabernardo).

## Refer√™ncias

- [Documenta√ß√£o do Pandas](https://pandas.pydata.org/docs/)
- [Documenta√ß√£o do PySpark](https://spark.apache.org/docs/latest/api/python/)
- [Documenta√ß√£o da GCP](https://cloud.google.com/docs)
- [Documenta√ß√£o do MySQL](https://dev.mysql.com/doc/)

